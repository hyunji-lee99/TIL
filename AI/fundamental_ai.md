# 인공지능/머신러닝 개론
머신러닝은 빅데이터를 분석할 수 있는 강력한 툴, 기존 통계학 및 시각화 방법의 한계를 해결함.
e.i) 예측, 패턴파악, 추천시스템 등..
명시적으로 프로그래밍을 하지 않고도 컴퓨터가 학습할 수 있는 능력을 갖게하는 것임. 
1. 지도학습 - 회귀분석, 분류** : 예측해야 할 결과에 대학 **정답 데이터가 있음**. e.i) 사진의 물체를 예측하는 모델을 만드는 경우
2. 비지도학습 : 예측해야 할 결과에 대한 **정답 데이터가 없음**. 
3. 강화학습 : 학습을 수행하기 위한 데이터가 없어도 학습이 가능함. 기계가 스스로 학습할 데이터를 만들어냄. 기계가 만든 데이터로 학습을 수행함. e.i) 최신 알파고-> 혼자 바둑두고 혼자 싸우고 ..
세 가지 종류가 있음. 

**자료의 형태**
데이터가 어떻게 구성되어 있는지, 어떤 머신러닝 모델을 사용해야 하는지, 데이터 전처리를 어떻게 해야하는지 결정하는 데에 자료의 형태를 먼저 파악해야 함. 
1. 범주형 자료 - 질적 자료로 불리며, 수치로 측정이 불가능한 자료 e.i) 성별, 지역, 혈액형 등
순위형 자료 : 범주 사이의 순서에 의미가 있음. e.i) 학점(에이플, 에이제로, 비플 등..) 
명목형 자료 : 범주 사이의 순서에 의미가 없음. e.i) 혈액형
다수의 범주가 반복해서 관측, 관측값의 크기보다 포함되는 범주에 관심 -> 범주형 자료 요약 필요
 1.각 범주에 속하는 관측값의 개수를 측정하고, 2. 전체에서 차지하는 각 범주의 비율 파악, 3. 효율적으로 범주 간의 차이점을 비교 가능.
 e.i) 도수분포표(범주, 도수-각 범주에 속하는 관측값의 개수 value_counts(), 상대도수-도수를 자료의 전체 개수로 나눈 비율 value_counts(normalize=True), 누적상대도수로 구성), 막대그래프(도수분포표에서 각 범주들의 크기 차이를 막대로 시각화,plt.bar())

2. 수치형 자료 - 양적 자료로 불리며, 수치로 측정이 가능한 자료 e.i) 키, 몸무게, 나이, 시험 점수 등
연속형 자료 : 연속적인 관측값을 가짐. e.i) 원주율(3.14159...), 시간(09:12:23.21...) 등 수학적으론 실수값이라고 할 수 있음.
이산형 자료 : 셀 수 있는 관측값을 가짐. e.i) 뉴스 글자 수, 주문 상품 개수 등..
범주형 자료와 달리 수치로 구성되어 있기에 **통계값**을 사용한 요약이 가능. 시각적 자료로는 이론적 근거 제시가 쉽지 않는 단점을 보완함. 많은 양의 자료를 의미 있는 수치로 요약하여 대략적인 분포상태를 파악 가능
e.i) 평균(관측값들을 대표할 수 있는 통계값, 수치형 자료의 통계값 중 가장 많이 사용되는 방법, 모든 관측값의 합을 자료의 개수로 나눈 것. 모든 관측값의 합계를 총 자료의 개수로 나눈 것이며 극단적으로 큰 값이나 작은 값에 영향을 받음), 퍼진 정도의 측도(평균만으로 분포를 파악하기에 부족, 평균 외에 분포가 퍼진 정도를 측도할 수치가 필요. 분산을 사용함! 분산은 자료가 얼마나 흩어졌는지 숫자로 표현, 각 관측값이 자료의 평균으로부터 떨어진 정도이며 값이 클수록 평균으로부터 많이 떨어져서 넓게 분포된 것, variance()) 
표준편차(분산의 양의 제곱근, 관측값과 단위를 일치시키기 위해 사용함. stdev()), 히스토그램(plt.hist(), 수치형 자료를 일정한 범위를 갖는 범주로 나누고 막대그래프와 같은 방식으로 그림. x축은 계급, y축은 빈도)
@범주형 자료와 수치형 자료를 자료의 숫자 표현 가능 여부로 구분하면 안됨. 
범주형 자료가 숫자로 표현된 경우로 남녀 성별 구분시, 여자0 남자1로 표현하는 경우 숫자로 표현되어 있으나 범주형 자료임. 
수치형 자료를 범주형 자료로 변환하는 경우로 나이 구분시, 나이 값은 수치형 자료지만 10-19세, 20-29세 등 나이 대에 따라 구간화하면 범주형 자료임. 

drink_freq = drink[drink["Attend"]==1]["Name"].value_counts() ->attend가 1인 애들을 각 name별로 도수분표를 할 때.
plt.hist(coffee, bins=3) -> 여기서 bins는 히스토그램을 구성하는 계급의 개수를 의미함. 즉, 도수를 계산하는 범주를 3개로 나눠서 히스토그램을 그릴 수 있음

**머신러닝을 위한 데이터 전처리**
머신러닝 과정
1. 데이터 수집 : 크롤링 또는 데이터베이스 데이터를 통하여 데이터 수집
2. 데이터 분석 및 전처리 : 수집한 데이터를 분석하고 머신러닝에 사용할 형태로 전처리
3. 머신러닝 학습 : 머신러닝 모델을 사용하여 데이터를 학습
4. 머신러닝 평가 : 학습된 머신러닝 모델을 평가용 데이터를 사용하여 평가

데이터 전처리의 역할
1. 머신러닝의 입력 형태로 데이터 변환(특성 엔지니어링)
2. 결측값 및 이상치를 처리하여 데이터 정제
3. 학습용 및 평가용 데이터 분리

대부분의 머신러닝 모델은 숫자 데이터를 입력받음. 일반적으로 행렬 형태 입력으로, 이를 학습하여 머신러닝 모델 완성
실제 데이터는 다양한 형태(이미지, 자연어, 범주형, 시계열 데이터 등)로 존재. 때문에, 실제 데이터는 머신러닝 모델이 이해할 수 없는 형태로 되어 있음. 
전처리를 통해서 데이터의 결측값(NaN) 및 이상치를 처리해야 함.  
전처리를 통해서 학습용과 평가용 데이터를 분리해야 함. 원본 데이터가 150 샘플이라면, 학습용 데이터가 100샘플 평가용 데이터가 50샘플 정도로 분리해야 함.

**범주형 자료 전처리**
범주의 크기가 의미 없다면 명목형 자료 e.i) 승객아이디, 생존여부, 이름, 성별 등..
크기가 의미가 의미 있다면 순서형 자료 e.i) 객실 등급
명목형 자료는 수치 맵핑 방식, 더미 기법이 있음.
순서형 자료는 수치 맵핑 방식을 사용.
**명목형 자료 - 수치 맵핑** : 일반적으로 범주를 0 또는 1로 맵핑. 예를 들어, 성별 자료는 여자 0, 남자 1로 변환. 또, 생존여부를 생존 1, 사망 0으로 변환 등..꼭 0 또는 1로 변환하는 것이 아니라 -1,1 이나 0,100 등으로 설정할 수 있지만 모델에 따라 성능이 달라질 수 있음. 
3개 이상인 경우, 수치의 크기 간격을 같게 하여 수치 맵핑. 타이타닉 데이터의 embarked의 경우 s->0, q->1, c->3 
titanic = titanic.replace({'female':1,'male':0})
**명목형 자료 - 더미 기법** : 더미 기법을 사용하여 각 범주를 0 또는 1로 변환
성별 컬럼의 값이 0 또는 1로 작성 -> 성별 칼럼을 두가지 컬럼으로 더 만들어서 남자인 경우, 여자 0 남자 1 여자인 경우, 여자 1 남자 0 이런 식으로 작성
dummies = pd.get_dummies(titanic[['Embarked']])
**순서형 자료 - 수치 맵핑** : 수치에 맵핑하여 변환하지만, 수치 간 크기 차이는 커스텀 가능. 크기 차이가 머신러닝 결과에 영향을 끼칠 수 있음.
매우 많음, 없음, 조금 많음 -> 10, 0, 4 등으로 변환

**수치형 자료 전처리**
크기를 갖는 수치형 값으로 이루어진 데이터 
수치로 되어있기 때문에 머신러닝 입력으로 바로 사용할 수 있으나, 모델의 성능을 높이기 위해 데이터 전처리 필요
**수치형 자료- 스케일링**
변수 값의 범위 및 크기를 변환하는 방식, 변수간의 범위가 차이가 나면 사용함. 
1. 정규화 : 유독 한 데이터가 변수간의 차이가 크다보면 머신러닝 모델에 유독 큰 영향을 줄 수 있기 때문에 변환.
정규화 값 = (기존 데이터 - 데이터 최솟값)/(데이터 최댓값-데이터 최솟값) -> 0~1 사이로 변환됨.
data = (data-min(data))/(max(data)-min(data))
2. 표준화 : 데이터가 정규분포를 따르고 있을 때, 평균이 0, 표준편차가 1인 표준정규분포에 가깝게 변환함.
표준화 값 = (기존 데이터 - 평균)/표준편차
 data = (data-np.mean(data))/np.std(data)
 pandas의 std는 표준편차의 불편추정량 (unbiased estimator)로 계산하기에 분모에 N-1을 numpy의 std는 표준편차의 편의추정량 (biased estimator)로 계산하기에 분모에 N을 쓰기에 차이가 발생함.
**수치형 자료 - 범주화**
변수의 값보다 범주가 중요한 경우 사용함.
예를 들어, 시험점수 데이터를 평균 이상이면 1, 평균 이하면 0으로 변환하는 등 범주화시킴.

**데이터 정제 및 분리하기**
info() 함수로 데이터의 null값이 얼마나 많은지 확인할 수 있음
1. 결측값 처리하기
일반적인 머신러닝 모델의 입력 값으로 결측값을 사용할 수 없음. 따라서 null, none, nan 등의 결측값을 처리해야 함
**결측값이 존재하는 샘플 삭제** -> titanic_2 = titanic_1.dropna() 
**결측값이 많이 존재하는 변수 삭제** -> 과반수 이상이 결측치로 존재함. titanic_1 = titanic.drop(columns=['Cabin'])
**결측값을 다른 값으로 대체**(평균값, 중앙값, 머신러닝 예측값 등으로)

2. 이상치(outlier) 처리하기
이상치가 있으면 모델의 성능을 저하할 수 있음. 일반적으로 전처리과정에서 제거하며, 어떤 값이 이상치인지 판단하는 기준이 필요
**통계 지표(카이제곱 검정, IQR 지표 등)를 사용하여 판단**
**데이터 분포를 보고 직접 판단**
#(Age 값 - 내림 Age 값) 0 보다 크다면 소수점을 갖는 데이터로 분류합니다.
outlier = titanic_2[titanic_2['Age']-np.floor(titanic_2['Age']) > 0 ]['Age']
titanic_3 = titanic_2[titanic_2['Age']-np.floor(titanic_2['Age']) == 0 ]
**머신러닝 기법을 사용하여 이상치 분류**

3. 데이터 분리하기
머신러닝 모델을 평가하기 위해서 학습에 사용하지 않은 평가용 데이터가 필요함
약 7:3~8:2 비율로 학습용 평가용 데이터를 분리함.
지도학습 데이터  분리 -> feature데이터와 label데이터를 분리하여 저장함. feature 데이터는 label을 예측하기 위한 입력값, label 데이터는 예측해야 할 대상이 되는 데이터임. 공부시간 대비 시험점수 예측에서 공부시간이 feature, 시험점수가 label임. 타이타닉 데이터에선 객실등급, 성별, 나이 등이 feature, 생존여부가 label데이터로 작용할 수 있음.

#feature 데이터와 label 데이터를 분리합니다.
X = titanic_3.drop(columns=['Survived'])
y = titanic_3['Survived']
print('X 데이터 개수: %d' %(len(X)))
print('y 데이터 개수: %d' %(len(y)))

#학습용, 평가용 데이터로 분리합니다.
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)
-> sklearn 라이브러리의 train_test_split 을 사용해서 분리함. 리턴값이 네 가지로, X는 feature데이터의 학습데이터와 테스트데이터이며, y는 label데이터의 학습과 테스트 데이터임. test size는 0에서 1사이의 값으로 들어가며, 0.1을 넣으면 전체 데이터의 10%는 테스트 데이터로, 나머지는 학습용 데이터로 분리하라는 의미임. 만약 1 이상의 값을 넣게 되면 개수로 인식함. 예를 들어, 데이터가 100개가 있는데 test size에 30을 넣게되면 테스트 데이터는 30개가 됨. random_state는 전체데이터에서 학습데이터와 테스트 데이터를 비율에 맞추어 뽑는 랜덤시드값으로, 이 값을 같은 값으로 통일해주면 항상 같은 학습 데이터와 테스트 데이터 값을 뽑게 됨. 

# 지도학습-회귀
예측해야할 대상과 feature 데이터가 선형적인 관계를 가지고 있을 경우 사용하기 좋음. lable 데이터가 수치형으로 되어있는 데이터를 가장 잘 설명하는 모델을 찾아 입력값에 따른 미래 결과값을 예측하는 알고리즘. 
e.i) x: 평균 기온, y: 아이스크림 판매량
가정 : y=b0+b1x => 적절한 b0,b1를 찾자.(데이터를 가장 잘 설명하는 모델)
적절한 b0, b1 값 찾기 : 완벽한 예측은 불가능하기에 최대한 잘 근사해야 함. 각 데이터의 실제 값과 모델이 예측하는 값의 차이를 최소한으로 하는 선을 찾아야 함. 단순 선형 회귀 모델을 학습하며 차이를 최소한으로 하는 선을 찾는 방법을 알아봐야 함. 
## 단순 선형 회귀
데이터를 설명하는 모델을 직선 형태로 가정. (y=b0+b1x)
직선을 구성하는 b0(y절편)와 b1(기울기)를 구해야 함. 
데이터를 잘 설명한다는 것은 실제 정답과 내가 예측한 값과의 차이가 작다는 것. 실제 값과 예측 값의 차이의 합으로 비교하기에는 예외가 존재함. 즉, 합계가 0이어도 실제론 정확도의 차이가 있을 수 있다는 것. (e.i) 예측값-실제값이 0,-1,1이어도 합이 0이고, 0,0,0이어도 합이 0) -> 실제 값과 예측값의 차이의 제곱의 합으로 비교해보는 것이 이러한 문제점을 해결할 수 있음. 
Loss 함수 : 실제 값과 예측값 차이의 제곱의 합. 이 함수값을 줄이려면 직선을 구성하는 b0(y절편)와 b1(기울기) 값을 조절해야 함. 
로스함수의 크기를 작게하는 y절편과 기울기를 찾는 방법으론, **경사하강법(gradient descent)**, normal equation, brute force search 등이 있음.
**경사 하강법**은 로스함수 값이 제일 작게하는 절편과 기울기를 b0*, b1*라고 하면, 경사 하강법은 계산 한번으로 b0*,b1*을 구하는 것이 아니라 초깃값에서 점진적으로 구하는 방식.
초깃값이란 랜덤으로 b0와 b1을 만들고, 0 1->1 3->3 5->...->3 7과 같이 초깃값에서 업데이트해서 b0*, b1*에 가까워져야 함. 
**업데이트 방법**
1) b0,b1 값을 랜덤하게 초기화
2) 현재 b0.b1값으로 로스 값 계산
3) 현재 b0,b1 값을 어떻게 해야 로스 값을 줄일 수 있는지 알 수 있는 gradient(로스 값을 b0,b1에 대해서 편미분하면 나옴) 값 계산
4) gradient 값을 활용하여 b0,b1 값 업데이트
5) 로스 값의 차이가 거의 없어질 때까지 2-4번 과정을 반복(로스 값과 차이가 줄어들면 gradient 값도 작아짐.)
특징 :
1. 가장 기초적이나 여전히 많이 사용되는 알고리즘
2. 입력값이 1개인 경우에만 적용이 가능함
3. 입력값과 결과값의 관계를 알아보는 데 용이함
4. 입력값이 결과값에 얼마나 영향을 미치는지 알 수 있음
5. 두 변수 간의 관계를 직관적으로 해석하고자 하는 경우 활용 -> b1이 얼마나 큰지에 따라 입력값이 얼마나 영향을 미치는지, b0는 입력값에 관계없이 영향을 미치는 정도를 알 수 있음.  

단순 선형 회귀
기계학습 라이브러리 scikit-learn 을 사용하면 Loss 함수를 최소값으로 만드는 b0,b1을 쉽게 구할 수 있음
from sklearn.linear_model import LinearRegression

- 데이터 전처리
sklearn에서 제공하는 linearregression을 사용하기 위해 pandas dataframe으로 feature (x)데이터와 series 형태의 label(y) 데이터로 전처리해야 함. 또, x,y의 샘플의 개수는 같아야 함.
train_X = pd.DataFrame(X,columns=['X']) -> 컬럼명 X인 데이터프레임으로 변환해서 train_X에 저장
train_Y = pd.Series(Y)

- 학습하기
linearRegression을 사용하기 위해선 우선 해당 모델 객체를 불러와 초기화해야함. 
lrmodel = LinearRegression()
모델 초기화 후 전처리된 데이터를 사용해서 학습을 수행함. fit함수에 학습에 필요한 데이터를 입력해서 학습을 수행함.
lrmodel.fit(train_X, train_Y)
학습한 결과를 시각화
plt.scatter(X, Y) 
plt.plot([0, 10], [lrmodel.intercept_, 10 * lrmodel.coef_[0] + lrmodel.intercept_], c='r') => 0과 10을 이어주는 선을 나타내는데, 학습한 모델의 b0값이 lrmodel.intercept_, b1값이 lrmodel.coef[0]이므로 위와 같이 표현.
plt.xlim(0, 10) 
plt.ylim(0, 10) 
plt.title('Training Result')
plt.savefig("test.png") 

- 예측하기
LinearRegression을 사용해서 예측을 하려면 predict 함수를 사용함.
pred_X = lrmodel.predict(train_X)
predict 함수는 DataFrame 또는 numpy array인 X 데이터에 대한 예측값을 리스트로 출력함. **X 데이터는 데이터프레임 또는 numpy array 가능**

## 다중 선형 회귀
여러 개의 입력값으로 결괏값을 예측하고자 하는 경우 다중 선형 회귀를 사용해야 함.
입력값 x가 여러 개인 경우 활용할 수 있는 회귀 알고리즘으로, 각 개별 xi에 해당하는 최적의 bi를 찾아야 함.
y=b0+b1x1+b2x2+b3x3+...+bmxm
e.i) 평균기온 x1과 평균 강수량 x2에 따른 아이스크림 판매량을 예측하고자 함.
y=b0+b1x1+b2x2

단순 선형 회귀와 마찬가지로 loss 함수는 입력값과 실제값 차이의 제곱의 합으로 정의함. 
b0,b1,...,bm 값을 조정해서 loss 함수의 크기를 작게 함.

특징
1. 여러 개의 입력값과 결과값 간의 관계 확인 가능
2. 어떤 입력값이 결과값에 어떠한 영향을 미치는지 알 수 있음
3. 여러 개의 입력값 사이 간의 상관 관계(두 가지 것의 한쪽이 변화하면 다른 한쪽도 따라서 변화하는 관계)가 높을 경우 결과에 대한 신뢰성을 잃을 가능성이 있음
-데이터 전처리
X = df[['FB','TV','Newspaper']]
Y = df[['Sales']]
train_X, test_X, train_Y, test_Y = train_test_split(X, Y, test_size=0.2, random_state=42)

-학습하기
단순선형회귀모델과 같은 방시으로 LinearRegression을 사용할 수 있음.
lrmodel = LinearRegression()  /*다중 선형 회귀 모델을 초기화하고 학습*/
lrmodel.fit(train_X, train_Y)
beta_0 = lrmodel.intercept_ # y절편 (기본 판매량)
beta_1 = lrmodel.coef_[0] # 1번째 변수에 대한 계수 (페이스북)
beta_2 = lrmodel.coef_[1] # 2번째 변수에 대한 계수 (TV)
beta_3 = lrmodel.coef_[2] # 3번째 변수에 대한 계수 (신문)

-예측하기
pred_X = lrmodel.predict(test_X)
df1 = pd.DataFrame(np.array([[0, 0, 0], [1, 0, 0], [0, 1, 0], [0, 0, 1], [1, 1, 1]]), columns=['FB', 'TV', 'Newspaper'])
pred_df1 = lrmodel.predict(df1)
print('df1에 대한 예측값 : \n{}'.format(pred_df1))

## 회귀 평가 지표
목표를 얼마나 잘 작성했는지 정도를 평가해서 좋은 모델인지 평가함 -> 실제값과 모델의 예측값의 차이에 기반한 평가 방법 사용
1. RSS-단순오차
실제 값과 예측 값의 단순 오차 제곱의 합 , 값이 작을수록 모델의 성능이 좋음. 전체 데이터에 대한 실제 값과 예측 값의 오차 제곱의 총합
가장 간단한 평가방법으로 직관적인 해석이 가능함. 그러나 오차를 그대로 사용하기 때문에 **입력 값의 크기에 의존적임.** 절대적인 값과 비교가 불가능함. 
2. MSE, MAE- 절대적인 크기에 의존한 지표
mse는 평균 **제곱** 오차, rss에서 데이터 수만큼 나눈 값임. 작을수록 모델의 성능이 높다고 평가할 수 있음. mse는 지표이고, loss는 모델에서 줄여야 하는 값임. 회귀에선 loss와 mse의 계산이 같아보일 수 있으나, 분류와 같은 회귀가 아닌 모델에서 mse는 다른 방식으로 쓰일 수 있음. 이상치 즉, 데이터들 중 크게 떨어진 값에 민감함(mae에 비해서)
mae는 평균 **절댓값** 오차, 실제 값과 예측값의 오차의 절대값의 평균. 작을수록 모델의 성능이 높음. 변동성이 큰 지표와 낮은 지표를 같이 예측할 시 유용함. 
MSE_train = mean_squared_error(train_Y,pred_train)
MAE_train = mean_absolute_error(train_Y,pred_train)
가장 간단한 평가방법들로 직관적인 해석이 가능하지만 평균을 그대로 이용하기 때문에 입력 값의 크기에 의존적이며 절대적인 값과 비교가 불가능함. 
4. R^2(결정계수)
회귀 모델의 설명력을 표한하는 지표, 1에 가까울수록 높은 성능의 모델이라고 해석할 수 있음. R^2=1-(RSS/TSS)이고, TSS는 데이터 평균 값과 실제 값 차이의 제곱임. 
오차가 없을수록 1에 가까운 값을 갖음. 값이 0인 경우, 데이터의 평균 값을 출력하는 직선 모델을 의미함. 음수값이 나온 경우, 평균 값 예측보다 성능이 좋지 않음. 
R2_train = r2_score(train_Y,pred_train)

# 지도학습-분류
가정) 해외 여행 준비하고 있다고 가정, 항공 지연을 피하고자 하는데, 기상정보를 이용해서 항공 지연 여부를 예측한다면?
문제 정의 ) 데이터 : 과거 기상정보(풍속,x)과 그에 따른 항공 지연 여부(y), 목표 : 현재 풍속에 따른 항공 지연 여부 예측하기 => 예측해야 하는 정답값이 존재하기 때문에 지도 학습. 예측 값이 범주형이기 때문에 분류 알고리즘으로 해결해야 함.
**분류란?** 주어진 입력 값이 **어떤 클래스(label)에 속할지**에 대한 결과 값을 도출하는 알고리즘. 다양한 분류 알고리즘이 존재하며, 예측 목표와 데이터 유형에 따라 적용함.
Q. 항공 지연 문제 해결하기
풍속 4m/s를 기준으로 지연 여부를 나눠보자.
- 풍속 4m/s보다 크면 지연.
- 풍속 4m/s보다 작으면 지연 없음.
분류 알고리즘 종류
1. 트리 구조 기반 - **의사결정나무**, 랜덤포레스트,..
2. 확률 모델 기반 - 나이브 베이즈 분류기...
3. 결정 경계 기반 - 선형 로지기, 로지스틱 회귀 분류기, svm,..
4. 신경망 - 퍼셉트론, 딥러닝 모델,..
..

## 의사결정나무 - 모델구조
의사결정나무란?
스무고개와 같이 특정 질문들을 통해 정답을 찾아가는 모델
최상단의 뿌리마디에서 마지막 끝마디까지 아래 방향으로 진행.
**중간 마디(internal node) 추가하기**
1-1.5 => no, 1.5-3 => yes, 3- => no
기준을 하나 더 만들 땐 중간마디로 분리함. 
-2개 이상의 feature 데이터의 경우
중간 마디 추가해서 결정하는 부분을 추가
def binary_tree(data, threshold):    
    yes = []
    no = []    
    # data로부터 풍속 값마다 비교를 하기 위한 반복문
    for wind in data['풍속']:    
        # threshold 값과 비교하여 분리합니다.
        if wind > threshold:
            yes.append(wind)
        else:
            no.append(wind)
    # 예측한 결과를 DataFrame 형태로 저장합니다.
    data_yes = pd.DataFrame({'풍속': yes, '예상 지연 여부': ['Yes']*len(yes)})
    data_no = pd.DataFrame({'풍속': no, '예상 지연 여부': ['No']*len(no)})    
    return data_no.append(data_yes,ignore_index=True) //ignore_index는? 각각에 해당되는 인덱스가 붙게되는데, 합치면서 새롭게 인덱스를 만들어주는 속성.

## 의사결정나무 - 불순도
**의사결정나무 분리 기준 알아보기**
데이터의 불순도(impurity)를 최소화하는 구역으로 나누자!
**불순도란 다른 데이터가 섞여 있는 정도.**
불순도는 어떻게 측정할까? 
1. 지니 계수(gini index)
해당 구역 안에서 특정 클래스에 속하는 데이터의 비율을 모두 제외한 값. 즉, 다양성을 계산하는 방법
지니 불순도 :
gini index=1-(yes의 확률)^2-(no의 확률)^2
gini impurity=(1번째 자식 마디의 데이터 개수)/(부모 마디의 데이터 개수)Gini_1 + (2번째 자식 마디의 데이터 개수)/(부모 마디의 데이터 개수)Gini_2
불순도가 작을수록 좋은 것임. 
각 마디에서 지니 불순도를 계산해서 가장 낮은 불순도를 갖는 기준을 바탕으로 데이터 분리

의사결정나무의 깊이 = 중간노드의 개수
의사결정나무가 깊어질수록 세분화해서 나눌 수 있음.하지만 너무 깊은 모델은 과적합을 야기할 수 있음.  => 데이터에 따라 다를 수 있지만 너무 깊은 모델은 지양
학습데이터를 너무 세밀하게 분류하면 평가데이터에서 좋은 평가를 받지 못할 수 있음.

의사결정나무의 특징
1. 결과가 직관적이며, 해석하기 쉬움.
2. 나무 깊이가 깊어질수록 과적합(overfitting) 문제 발생 가능성이 매우 높음.
3. 학습이 끝난 트리의 작업 속도가 매우 빠름.

**sklearn을 이용한 의사결정나무 - 데이터 전처리**
#sklearn에 저장된 데이터를 불러 옵니다.
X, Y = load_iris(return_X_y = True)
#DataFrame으로 변환
df = pd.DataFrame(X, columns=['꽃받침 길이','꽃받침 넓이', '꽃잎 길이', '꽃잎 넓이'])
df['클래스'] = Y
X = df.drop(columns=['클래스'])
Y = df['클래스']
"""
1. 학습용 평가용 데이터로 분리합니다
"""
train_X, test_X, train_Y, test_Y = train_test_split(X, Y, test_size=0.2, random_state = 42)

**sklearn을 이용한 의사결정나무 -학습하기**
DecisionTreeClassifier를 사용함
max_depth를 이용해서 의사결정나무의 최대 깊이를 조절할 수도 있음.
e.i) DTmodel = DecisionTreeClassifier(max_depth=2)
#DTmodel에 의사결정나무 모델을 초기화 하고 학습합니다
DTmodel = DecisionTreeClassifier()
DTmodel.fit(train_X, train_Y)
#학습한 결과를 출력합니다
plt.rc('font', family='NanumBarunGothic')
fig = plt.figure(figsize=(25,20))
_ = tree.plot_tree(DTmodel, 
                   feature_names=['꽃받침 길이','꽃받침 넓이', '꽃잎 길이', '꽃잎 넓이'],  
                   class_names=['setosa', 'versicolor', 'virginica'],
                   filled=True)

**sklearn을 이용한 의사결정나무 -예측하기**
#test_X에 대해서 예측합니다.
pred_X = DTmodel.predict(test_X)
print('test_X에 대한 예측값 : \n{}'.format(pred_X))

## 분류 평가 지표
1. 혼동 행렬(confision matrix)
분류 모델의 성능을 평가하기 위함. 실제 행와 예측 열로 구분된 행렬로, TP, FN, FP, TN이 있음.
true positive : 실제 positive인 값을 positive라고 예측(정답)
true negative : 실제 negative인 값을 negative라고 예측(정답)
false positive : 실제 negative인 값을 positive라고 예측(오답) - 1형 오류
false negative : 실제 positive인 값을 negative라고 예측(오답) - 2형 오류
-> confusion_matrix(y_true, y_pred) : np.ndarray로 반환

2. 정확도(accuracy)
전체 데이터 중에서 제대로 분류된 데이터의 비율로, 모델이 얼마나 정확하게 분류하는지를 나타냄.  일반적으로 분류 모델의 주요 평가 방법으로 사용.
그러나, 클래스 비율이 불균형 할 경우 평가 지표의 신뢰성을 읽을 가능성이 있음.  
=> (TP+TF)/(P+F)
P=TP+FN, F=TN+FP
->DTmodel.score(train_X, train_Y)

3. 정밀도(precision)
모델이 positive라고 분류한 데이터 중에서 실제로 positive인 데이터의 비율
-negative가 중요한 경우.
=> TP/(TP+FP)
즉, 실제로 negative인 데이터를 positive라고 판단(FP)하면 안되는 경우 사용되는 지표. e.i) 스팸 메일 판결을 위한 분류 문제. 해당 메일이 스팸인 경우 positive, 스팸이 아닌 경우 negative인데 일반메일을 스팸메일 positive로 잘못 예측하면 중요한 메일을 전달받지 못할 상황이 발생할 수 있음.
-> precision_score(train_Y, y_pred_train)

4. 재현율(recall,tpr)
실제로 positive인 데이터 중에서 모델이 positive로 분류한 데이터의 비율
-positive가 중요한 경우
즉, 실제로 positive인 데이터를 negative라고 판단하면 안되는 경우 사용되는 지표 
=> TP/(TP+FN)=TP/P
-> recall_score(train_Y, y_pred_train)

분류 목적에 따라 다양한 지표를 계산하여 평가
1. 분류 결과를 전체적으로 보고싶다면 -> 혼동행렬
2. 정답을 얼마나 잘 맞췄는지 -> 정확도
3. FP 또는 FN의 중요도가 높다면 -> 정밀도 or 재현율

**혼동행렬 계산하기**
#sklearn에 저장된 데이터를 불러 옵니다.
X, Y = load_breast_cancer(return_X_y = True)
X = np.array(X)
Y = np.array(Y)

#데이터 정보를 출력합니다
print('전체 샘플 개수: ',len(X))
print('X의 feature 개수: ',len(X[0]))

#학습용 평가용 데이터로 분리합니다
train_X, test_X, train_Y, test_Y = train_test_split(X, Y, test_size=0.2, random_state = 42)

#분리된 평가용 데이터 정보를 출력합니다
print('평가용 샘플 개수: ',len(test_Y))
print('클래스 0인 평가용 샘플 개수: ',len(test_Y)-sum(test_Y))
print('클래스 1인 평가용 샘플 개수: ',sum(test_Y),'\n')

#DTmodel에 의사결정나무 모델을 초기화 하고 학습합니다
DTmodel = DecisionTreeClassifier()
DTmodel.fit(train_X, train_Y)

#test_X을 바탕으로 예측한 값을 저장합니다
y_pred = DTmodel.predict(test_X)

"""
1. 혼동 행렬을 계산합니다
"""
cm = confusion_matrix(test_Y,y_pred)
print('Confusion Matrix : \n {}'.format(cm))

#혼동 행렬을 출력합니다
fig = plt.figure(figsize=(5,5))
ax = sns.heatmap(cm, annot=True)
ax.set(title='Confusion Matrix',
            ylabel='True label',
            xlabel='Predicted label')
fig.savefig("decistion_tree.png")

**정확도 계산하기**
acc_train = DTmodel.score(train_X,train_Y)
train_X에 대한 정확도 계산
acc_test = DTmodel.score(test_X,test_Y)
test_X에 대한 정확도 계산

**정밀도와 재현율 계산하기**
"""
1. 정밀도를 계산합니다.
"""
precision_train = precision_score(train_Y, y_pred_train)
precision_test = precision_score(test_Y, y_pred_test)

#정밀도를 출력합니다.
print('train_X Precision: %f' % (precision_train))
print('test_X Precision: %f' % (precision_test),'\n')

"""
2. 재현율을 계산합니다.
"""
recall_train = recall_score(train_Y,y_pred_train)
recall_test = recall_score(test_Y,y_pred_test)

#재현율을 출력합니다.
print('train_X Recall: %f' % (recall_train))
print('test_X Recall: %f' % (recall_test))
